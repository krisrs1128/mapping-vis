{
  "articles": [
    {
      "path": "1-preview.html",
      "title": "A First Look",
      "description": "An initial look at the raw data, with an eye towards preprocessing.\n",
      "author": [
        {
          "name": "Kris Sankaran",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nRstudio Link\nIn this notebook, we take a first look at the glacier data. We’ll download the raw annotations and images, using visualization to gain familiarity with the structure of the problem.\nWe’ll be using the R packages below. dplyr, ggplot2, and tidyr are part of R’s tidy data ecosystem. gdalutils, raster, and sf are packages for general spatial data manipulation. RStoolbox gives useful functions for spatial data visualization specifically.\n\n\nlibrary(\"RStoolbox\")\nlibrary(\"dplyr\")\nlibrary(\"gdalUtils\")\nlibrary(\"ggplot2\")\nlibrary(\"raster\")\nlibrary(\"readr\")\nlibrary(\"sf\")\nlibrary(\"tidyr\")\nsource(\"data.R\")\ntheme_set(theme_bw())\n\n\n\nIn the block below, we’ll create a directory structure to store the data in our analysis. If you’re working on your own computer, you can change the directory to which files are written by modifying the params block in the header of this rmarkdown file.\n\n\ndata_dir <- params$data_dir\ndir.create(params$data_dir, recursive = TRUE)\n\n\n\nNext, we download the actual imagery. The data are currently stored on a UW Madison Box folder. We first download a file containing all the raw data links1, and then loop over just those links. This step can take some time, because the files are relatively large.\n\n\n# download all linked data\ndata_links <- read_csv(params$data_links) %>%\n  filter(download)\n\nfor (i in seq_len(nrow(data_links))) {\n  download.file(\n    data_links$link[i], \n    file.path(data_dir, data_links$name[i])\n  )\n}\n\n\n\nOften, we’ll want to find the image associated with a particular point on the earth. In the current form, this is quite hard to do: we’d have to check image by image, until we found the one that had the geographic coordinate that we’re looking for. A much more convenient format is something called a ``Virtual Tiff.’’ This indexes all the imagery we had originally downloaded, letting us look up imagery just by specifying the geographic coordinates. This means we can hide all the complexity of working with many imagery files and pretend we had a single (very large) image.\n\n\nx_paths <- dir(data_dir, \"*tiff\", full.names = TRUE)\nvrt_path <- file.path(data_dir, \"region.vrt\")\ngdalbuildvrt(x_paths, vrt_path, ot=\"float64\")\n\n\nNULL\n\nTime for our first visualization! The glaciers_small.geojson file demarcates the boundaries for two types of glaciers that are common in the Hindu Kush Himalayas region. The full version, available in the box folder, gives coordinates for glaciers across the whole region, which spans many countries, but reading this into Binder would put us against some of the compute limits. Instead, let’s pick one a small patch from one of 63 basins in the region and visualize its glacier boundaries.\n\n\ny <- read_sf(file.path(data_dir, \"glaciers_small.geojson\")) %>%\n  filter(\n    Sub_basin == \"Dudh Koshi\",\n    Longitude < 86.91, Longitude > 86.1,\n    Latitude < 27.81, Latitude > 27.7\n  )\n\nhead(y)\n\n\nSimple feature collection with 6 features and 20 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: 86.58008 ymin: 27.69711 xmax: 86.8944 ymax: 27.80701\ngeographic CRS: WGS 84\n# A tibble: 6 x 21\n     ID Longitude Latitude GLIMS_ID Glaciers Elv_min Elv_mean Elv_max\n  <int>     <dbl>    <dbl> <chr>    <chr>      <int>    <int>   <int>\n1   193      86.9     27.8 G086869… Debris …    4885     5035    5366\n2    23      86.6     27.8 G086590… Debris …    4893     4969    5367\n3     5      86.6     27.7 G086578… Debris …    4430     4627    5216\n4   197      86.9     27.8 G086851… Debris …    5349     5434    5509\n5   210      86.9     27.7 G086865… Debris …    4472     4589    4698\n6   176      86.8     27.8 G086796… Debris …    4344     4659    5015\n# … with 13 more variables: Slope_min <int>, Slope_mean <int>,\n#   Slope_max <int>, Aspect <int>, Area_SqKm <dbl>, Thickness <dbl>,\n#   Reserve <dbl>, Basin <chr>, M_Basin <chr>, Region <chr>,\n#   Country <chr>, Sub_basin <chr>, geometry <MULTIPOLYGON [°]>\n\n\n\nggplot(y, aes(fill = Glaciers)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"#93b9c3\", \"#4e326a\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFrom a technical perspective, there are two points worth noting. First, we were able to use dplyr’s filter function on these glaciers, as if they were stored in an ordinary data frame (they’re in fact stored in a spatial data frame). Being able to use concepts of tidy data manipulation in spatial problems can make our lives much easier. Second, notice that we’re using tm_shape from the tmap package – this package makes much nicer spatial data visualizations than simply calling plot() from base R. It also implements a grammar of graphics for spatial data, allowing us to layer on different visual encodings in a flexible way.\nNext, let’s visualize the satellite imagery in that region. The satellite images are not like ordinary images from a camera – they have many sensors. In our case, each pixel is associated with 15 measurements. For example, the block below first plots the RGB colors associated with the region before showing a composite that turns all the glaciers blue.\n\n\nx <- read_subset(vrt_path, st_bbox(y))\nggRGB(x)\n\n\n\nggRGB(x, r = 5, g = 4, b = 2)\n\n\n\n\nNotice the similarities with the outlines in the glaciers.geojson file. We can more or less distinguish the clean ice and debris-covered glaciers from the rest of the image. Looking a bit more carefully, though, we realize that there are lots of areas that fall into the bluish regions but which aren’t labeled as clean ice glacier. We might hope that some of the other channels make the difference more visible, but it seems like there might be some danger of false positives. Also, it seems like the debris-covered glaciers are only a subtley different color from the background – their tendril-like shape is a much better visual clue.\nWe’ve managed to use geom_sf and ggRGB to avoid having to convert the raw data to standard R data frames. There are some situations, however, when it can be useful to make that conversion. The code below provides an example that extracts the slope channel from the satellite image and visualizes it using standard ggplot2. It seems like the debris-covered glaciers are relatively flat.\n\n\nslope <- subset(x, 15) %>%\n  as.data.frame(xy = TRUE)\n\nggplot(slope, aes(x = x, y = y)) +\n  geom_raster(aes(fill = slope)) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  coord_fixed() +\n  scale_x_continuous(expand = c(0, 0)) + \n  scale_y_continuous(expand = c(0, 0))\n\n\n\nrm(slope) # save space\n\n\n\nLet’s make a few histograms, one for each satellite sensor. From this view, it becomes clear that we’re going to have to preprocess these data before modeling. First, the ranges across sensors can vary substantially. Second, the BQA channels is not actually a sensor measurement – it’s a quality assignment, which is why it takes on so few values.\n\n\nsample_ix <- sample(nrow(x), 100)\nx_df <- x[sample_ix, sample_ix, ] %>% # subset pixels\n  as.data.frame()\nx_longer <- x_df %>%\n  pivot_longer(cols = everything())\n\nggplot(x_longer) +\n  geom_histogram(aes(x = value)) +\n  facet_wrap(~ name, scale = \"free_x\")\n\n\n\n\nFor our final view, let’s make a scatterplot of a few channels against one another. We’ll bin using geom_hex, because otherwise the points overlap too much. This visualization makes it clear how the two B6 channels are nearly copies of one another, so we can safely drop one of them from our visualization. For reference, we also plot the first two channels, B1 and B2, against one another.\n\n\nggplot(x_df, aes(x = B6_VCID_1, y = B6_VCID_2, fill = log(..count..))) +\n  geom_hex(binwidth = 2) +\n  scale_fill_viridis_c() +\n  coord_fixed()\n\n\n\nggplot(x_df %>% filter(B1 != 255, B2 != 255), aes(x = B1, y = B2, fill = log(..count..))) +\n  geom_hex(binwidth = 2) +\n  scale_fill_viridis_c() +\n  coord_fixed()\n\n\n\n\nWe haven’t made too many plots, but we’ve already gained some valuable intuition about (i) the types of visual features that might be useful for identifying the two types of glaciers and (ii) some properties of the data that, if not properly accounted for, could be disastrous for any downstream modeling (no matter how fancy the model).\n\nIt’s very meta.↩︎\n",
      "last_modified": "2021-02-15T18:28:05-06:00"
    },
    {
      "path": "2-preprocess.html",
      "title": "Data Preparation",
      "description": "Generating data for model training.\n",
      "author": [
        {
          "name": "Kris Sankaran",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nRstudio Link\nIn these notes, we generate and visualize patches of data that will be used to train the mapping model. This is necessary for a few different reasons,\nPreprocessing: The different channels need to be normalized, since they have such different ranges. There are also a few channels that we should drop, like the BQA quality channel we saw earlier.\nImbalance: The glaciers only make up a relatively fraction of the total area that we have imagery for. Our model can be trained more efficiently by zooming into the parts of the region that actually have glaciers.\nImage size: Even if we completed these preprocessing steps, each satellite image is much larger than anything a machine learning algorithm could work with. We’ll need to break the processed imagery into pieces that can be sequentially streamed in for training the model1\nWe load quite a few libraries for this step. Many will be familiar from the previous notes, but two new ones are abind, which is used to manipulate subsetted arrays of imagery, and reticulate, which is used to navigate back and forth between R and python. We need reticulate because we will save our final dataset as a collection of .npy numpy files – these are a convenient format for training our mapping model, which is written in python.\n\n\nlibrary(\"RStoolbox\")\nlibrary(\"abind\")\nlibrary(\"dplyr\")\nlibrary(\"gdalUtils\")\nlibrary(\"ggplot2\")\nlibrary(\"gridExtra\")\nlibrary(\"purrr\")\nlibrary(\"raster\")\nlibrary(\"readr\")\nlibrary(\"reticulate\")\nlibrary(\"sf\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\n\n# setting up python environment\nuse_condaenv(\"notebook\")\nnp <- import(\"numpy\")\nsource(\"data.R\")\ntheme_set(theme_minimal())\nset.seed(123)\n\n\n\nWe want to make sure we don’t overfit to any particular region. To this end, we’ll use different geographic sub-basins for training and evaluation. For training, we’re just using the Kokcha basin, and for evaluation, we use Dudh Koshi. In general, our notebook takes arbitrary lists of basins, specified by links to csv files through the basins parameter in the header. In practice, a larger list of training basins would be used to train the model, but working with that is much more computationally intensive.\n\n\ny_path <- file.path(params$raw_dir, \"glaciers_small.geojson\")\nbasins <- read_csv(params$basins)\n\ny <- read_sf(y_path) %>%\n  filter(Sub_basin %in% basins$Sub_basin)\n\n\n\nTo address the imbalance and image size issues, we’ll sample locations randomly from within the current basins’ glaciers. This is done using the st_sample function. More patches will translate into more patches for training the model, but it will also increase the chance that training patches overlap. You will see a warning message about st_intersection – it’s safe to ignore that for our purpose (we are ignoring the fact that the surface of the earth is slightly curved).\n\n\ncenters <- y %>%\n  st_sample(params$n_patches, type = \"random\", by_polygon = FALSE) %>%\n  st_coordinates()\ncolnames(centers) <- c(\"Longitude\", \"Latitude\")\n\n\n\nTo better understand this sampling procedure, let’s visualize the centers of the sampled locations on top of the basins that are available for training. We can see that we have more samples in areas that have higher glacier density, which helps alleviate the imbalance problem. As an aside, this visualization gives an example of visualizing a spatial geometry (the glaciers object, y) together with an ordinary data frame (the centers for sampling).\n\n\np <- ggplot(y, aes(x = Longitude, y = Latitude)) +\n  geom_sf(data = y, aes(fill = Glaciers)) +\n  geom_point(data = as.data.frame(centers), col = \"red\", size = 2) +\n  scale_fill_manual(values = c(\"#93b9c3\", \"#4e326a\"))\n\np\n\n\n\n\nThat image is quite zoomed out. To see some of the sampling locations along with just a few glaciers, we can zoom in, using the coord_sf modifier.\n\n\np + coord_sf(xlim = c(70.7, 71.2),  ylim = c(36.2, 36.5))\n\n\n\n\nNow that we know where we want to sample our training imagery, let’s extract a patch. This is hidden away in generate_patch in the data.R script accompanying this notebook. This function also does all the preprocessing that we mentioned in the introduction. We’ll see the effect of this preprocessing in a minute – for now, let’s just run the patch extraction code. Note that we simultaneously extract a corresponding label, stored in patch_y. It’s these preprocessed satellite imagery - glacier label pairs that we’ll be showing to our model in order to train it.\n\n\nvrt_path <- file.path(params$raw_dir, \"region.vrt\")\nys <- y %>% split(.$Glaciers)\n\n\n\nLet’s take a look at the preprocessed patches, just as a sanity check. We’re plotting the first of the sampled patches below. The image is much smaller now, but still contains a fair amount of glacier. Notice the false negative debris-covered glacier along the top of the image! A more sophisticated model would account for these kinds of data-specific variations, which become obvious when you visualize the data, but which are otherwise very hard to recognize.\n\n\npatch <- generate_patch(vrt_path, centers[5, ])\npatch_y <- label_mask(ys, patch$raster)\np <- list(\n  plot_rgb(brick(patch$x), c(5, 4, 2), r = 1, g = 2, b = 3),\n  plot_rgb(brick(patch$x), rep(13, 3)),\n  plot_rgb(brick(patch_y), r = NULL)\n)\ngrid.arrange(grobs = p, ncol = 3)\n\n\n\n\nThe other major change in these preprocessed images is that we’ve applied a linear transformation to each channel, mapping the sensor measurements to between \\(\\left[-1, 1\\right]\\) for every image. We’re using the same histogram code that we used in the first notebook.\n\n\nsample_ix <- sample(nrow(patch$x), 100)\nx_df <- patch$x[sample_ix, sample_ix, ] %>%\n  brick() %>%\n  as.data.frame() %>%\n  pivot_longer(cols = everything())\n\nggplot(x_df) +\n  geom_histogram(aes(x = value)) +\n  facet_wrap(~ name, scale = \"free_x\")\n\n\n\n\nNow that we’ve checked one of the patches, we can write them all to numpy arrays. Even after subsetting to only one basin, this step takes a fair bit of time, so we’ll instead just refer to training and test patches that I generated earlier. We’ll be downloading them at the start of the next notebook, which uses these data to train a mapping model. If you’re curious, we’ve also generated patches using a large list of training basins, available here. Using this larger dataset leads to a noticeably better model, but makes for an unwieldy tutorial. Nonetheless, the data are available for your experimentation after the workshop.\n\n\n#write_patches(vrt_path, ys, centers, params$out_dir)\n#unlink(params$raw, recursive = TRUE)\n\n\n\n\nFor reference, the ImageNet dataset, which is a standard benchmark for computer vision problems, most images are usually cropped to 256 \\(\\times\\) 256 pixels.↩︎\n",
      "last_modified": "2021-02-15T18:28:06-06:00"
    },
    {
      "path": "3-train.html",
      "title": "Model Training",
      "description": "Train a model for classifying pixels into different glacier types, using the\npytorch package.\n",
      "author": [
        {
          "name": "Kris Sankaran",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nCode Link\nIn these notes, we’ll train a segmentation model to predict glacier, given the preprocessed patches created by the previous notes. Since we didn’t have enough time to preprocess all the training data in the previous notebook, we use the code block below to download and extract patches precomputed in advance. Note that there are only 67 patches in this archive – this keeps the download relatively quick, and keeps us from getting in trouble with the people hosting these binder notebooks.\nimport urllib.request\nimport tarfile\nfrom pathlib import Path\nfrom data import create_dir, download_data\nimport os\n\n# setup directory structure for download\ndata_dir = Path(\"/home/jovyan/data\")\nprocess_dir = data_dir / \"processed\"\ncreate_dir(process_dir)\n\n# download processed data\ndownload_data(\n    \"https://uwmadison.box.com/shared/static/d54agxzb5g8ivr7hkac8nygqd6nrgrqr.gz\", \n    process_dir / \"train.tar.gz\"\n)\nThe block below specifies some parameters of our learning algorithm, a U-Net segmentation model. Like most deep learning algorithms, we train it using a variant of stochastic optimization. The lr parameter below refers to the optimizer’s learning rate. The binder notebooks we’re running off of don’t have GPUs. If they did, we could set device: \"cuda\", and we’d be able to train the model much faster. We also had to limit the batch size, to avoid going over the memory limit imposed on these online notebooks.\nargs = {\n    \"batch_size\": 1, # make this bigger if you are not running on binder\n    \"epochs\": 50,\n    \"lr\": 0.0001,\n    \"device\": \"cpu\" # set to \"cuda\" if GPU is available\n}\nOur optimizer is going to need a way to stream in the preprocessed patches. This is accomplished using the DataLoader object below. If you tried visualizing the items in the data loader, you would see the same image-label pairs from the previous notebook. This step might seem mysterious if you haven’t used a deep learning algorithm before. I’m deliberately avoiding an extended discussion on deep learning – my emphasis here is on visualization and earth observation. There are also many good references on applied deep learning already.\nfrom data import GlacierDataset\nfrom torch.utils.data import DataLoader\n\npaths = {\n    \"x\": list((process_dir / \"train\").glob(\"x*\")),\n    \"y\": list((process_dir / \"train\").glob(\"y*\"))\n}\n\nds = GlacierDataset(paths[\"x\"], paths[\"y\"])\nloader = DataLoader(ds, batch_size=args[\"batch_size\"], shuffle=True)\nGiven a way of loading the training data, we can train our model. The parameters in the definition of the Unet correspond to 13 input sensor channels, 3 output classes (clean-ice glacier, debris-covered glacier, and background), and 4 layers.\nWe can try running the model below, but it will not finish in the time for this workshop (Though, with a larger batch size and a GPU, it doesn’t take too long to converge.). We’ll instead download a model that I already trained earlier. This model was also trained using all the training patches, and not just those from the Kokcha basin.\nimport torch.optim\nfrom unet import Unet\nfrom train import train_epoch\n\nmodel = Unet(13, 3, 4, dropout=0.2).to(args[\"device\"])\noptimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n\nfor epoch in range(args[\"epochs\"]):\n    train_epoch(model, loader, optimizer, args[\"device\"], epoch)\n    \ntorch.save(model.state_dict(), data_dir / \"model.pt\")\n\n\n\n",
      "last_modified": "2021-02-15T18:28:06-06:00"
    },
    {
      "path": "4-save.html",
      "title": "Saving Predictions",
      "description": "Save predictions from python to R, for subsequent visualization.\n",
      "author": [
        {
          "name": "Kris Sankaran",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nCode Link\nOnce we have trained a model, it’s important to visualize its predictions. We’re going to use R to make our visualizations, but our model has been saved as a python object. This script helps with the transition betwen languages, saving all the model’s predictions as numpy arrays, which can be read in R using the reticulate package. We’ll make predictions on both training and test data, to gauge the degree of over / underfitting.\nThe block below defines some high-level parameters for this script. If you are running this on your own machine, you should change the data_dir parameter to whereever you have been storing the raw and processed data. Also, if you have access to a GPU, you should change the device parameter, since it would help us get the predictions more quickly.\nfrom pathlib import Path\n\ndata_dir = Path(\"/home/jovyan/data\")\nprocess_dir = data_dir / \"processed\"\nargs = {\n    \"device\": \"cpu\", # set to \"cuda\" if gpu is available\n    \"out_dir\": data_dir / \"predictions\"\n}\nWe left the last notebook without fully training the model. We also never generated the test data that would have been made by the 2-preprocessing.Rmd script before. Instead, in this block, we will download a test data set and trained model, currently stored in a UW Madison box folder.\nfrom data import download_data\n\nlinks = {\n    \"test_data\": \"https://uwmadison.box.com/shared/static/zs8vtmwbl92j5oq6ekzcfod11ym1w599.gz\",\n    \"model\": \"https://uwmadison.box.com/shared/static/byb5lpny6rjr15zbx28o8liku8g6nga6.pt\"\n}\n\ndownload_data(links[\"test_data\"], process_dir / \"test.tar.gz\")\ndownload_data(links[\"model\"], data_dir / \"model.pt\", unzip = False)\nThis block sets up the model that we just downloaded. The .eval() step specifies that we are no longer using the model for training. We don’t need to keep track of model gradients anymore, since all we care about are predictions made with the existing weights.\nimport torch\nfrom unet import Unet\n\nstate = torch.load(data_dir / \"model.pt\", map_location=args[\"device\"])\nmodel = Unet(13, 3, 4).to(args[\"device\"])\nmodel.load_state_dict(state)\nmodel = model.eval()\nThe block below creates Dataset objects from which we can load the preprocessed training and test samples. We rely on the fact that our directory structure completely species the train / test split. We will iterate over these images one by one, saving a prediction for each. In principle, it’s possible to save predictions over batches of images by first defining a data loader. This would be a bit more complex to implement, though, and we’re aiming for simplicity here.\nfrom data import GlacierDataset\nfrom torch.utils.data import DataLoader\n\npaths = {}\nfor split in [\"train\", \"test\"]:\n    paths[split] = {}\n    for v in [\"x\", \"y\"]:\n        paths[split][v] = list((process_dir / split).glob(v + \"*\"))\n        paths[split][v].sort()\n\nds = {\n    \"train\": GlacierDataset(paths[\"train\"][\"x\"], paths[\"train\"][\"y\"]),\n    \"test\": GlacierDataset(paths[\"test\"][\"x\"], paths[\"test\"][\"y\"])\n}\nFinally, we save predictions to the args[\"out_dir\"] folder. The code for the predictions function is given in the train.py script. It iterates over the loader and saves a numpy array with predictions for each sample. Somewhat counterintuively, we also save the x and y’s associated with each prediction. The reason is that the output from the Dataset object is not deterministic – we may return a random rotation or flip of the original image. This was done to encourage invariance to these transformations in our model, but makes it hard to compare the predictions directly with the objects in the processed directory. By writing all the matched input, label, and prediction data again at this point, we make it easier to study the specific version of the inputs that are related to good and bad model performance.\nfrom train import predictions\n\npredictions(model, ds[\"train\"], args[\"out_dir\"] / \"train\", args[\"device\"])\npredictions(model, ds[\"test\"], args[\"out_dir\"] / \"test\", args[\"device\"])\n\n\n\n",
      "last_modified": "2021-02-15T18:28:06-06:00"
    },
    {
      "path": "5-eval.html",
      "title": "Model Evaluation",
      "description": "Studying saved predictions from a model.\n",
      "author": [
        {
          "name": "Kris Sankaran",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nRstudio\nIn these notes, we’ll visualize some of the predictions from our trained model. This will help us develop a more nuanced understanding of model quality, rather than average test set error alone. Indeed, while training and test errors are useful for diagnosing whether a model is over or underfitting a dataset, they alone don’t tell us when a model is most likely to make mistakes.\nAs usual, our first step is to load libraries that will be useful in this analysis. By this point, we have seen most of this mix of spatial and tidy data analysis packages. We use the reticulate package to read the raw predictions, which were saved as numpy arrays, into R arrays.\n\n\nlibrary(\"RStoolbox\")\nlibrary(\"raster\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gridExtra\")\nlibrary(\"purrr\")\nlibrary(\"reticulate\")\nlibrary(\"sf\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nuse_condaenv(\"notebook\")\nnp <- reticulate::import(\"numpy\")\nsource(\"data.R\")\nsource(\"metrics.R\")\ntheme_set(theme_minimal())\n\n\n\nWe will download predictions that would have been saved by the save_preds.ipynb script, if we had let it run for a few more minutes. The download still takes time (the file is a little over 700MB), but it is still faster than waiting for the predictions to be saved.\n\n\npreds_dir <- params$preds_dir\ndir.create(preds_dir)\npreds_file <- file.path(preds_dir, \"preds.tar.gz\")\ndownload.file(\"https://uwmadison.box.com/shared/static/5s7sqvh50iy5p2yl2basfdzgss9lgnxr.gz\", preds_file)\nuntar(preds_file, exdir = params$preds_dir)\nunlink(preds_file)\n\n\n\nThe downloaded data include more than the predictions y_hat – they also include the saved patches x and ground truth labels y. We have this for both training and test regions. To manage all these different files, let’s create a data frame that helps keep track of which types of files are where.\n\n\npaths <- prediction_paths(preds_dir)\nhead(paths)\n\n\n                                            path split  type ix\n1     /Users/kris/data/predictions//test/x-0.npy  test     x  0\n2     /Users/kris/data/predictions//test/y-0.npy  test     y  0\n3 /Users/kris/data/predictions//test/y_hat-0.npy  test y_hat  0\n4     /Users/kris/data/predictions//test/x-1.npy  test     x  1\n5     /Users/kris/data/predictions//test/y-1.npy  test     y  1\n6 /Users/kris/data/predictions//test/y_hat-1.npy  test y_hat  1\n\nHow good are the predictions? Our setting is a little trickier than the usual regression or classification settings. This is because (i) for each sample, we make predictions at every single pixel and (ii) we have several classes of interest. For (i), the idea is to evaluate the precision and recall associated with the entire patch as we vary the thresholds used to demarcate classes. See the next point. For (ii), we make sure to compute these precision and recall statistics for every single class.\nTo illustrate (i) from the previous point, consider the two images below, giving the ground truth and predicted probabilities for the clean-ice glacier class, for one patch.\n\n\ny <- load_npy(paths$path[2])\ny_hat <- load_npy(paths$path[3])\np <- list(\n  plot_rgb(y, 1, r = NULL, g = NULL, b = 1),\n  plot_rgb(y_hat, 1, r = NULL, g = NULL, b = 1)\n)\ngrid.arrange(grobs = p, ncol = 2)\n\n\n\n\nTo assign a class to every pixel, we would need to declare probability thresholds, above which the pixel belongs to the target class (e.g., clean ice glacier) and below which it is considered background. The for loop below varies the threshold, going from lenient (high glacier recall, but low precision) to strict (low glacier recall, but high precision).\n\n\nthresholds <- c(seq(0.05, 0.95, 0.01), rep(0.95, 10))\nfor (i in seq_along(thresholds)) {\n  print(plot_rgb(y_hat > thresholds[i], 1, r = NULL, g = NULL, b = 1))\n}\n\n\n\n\nThe block below computes precision and recall across a range of thresholds, for each sample in the training and test splits. This generates new diagnostic data at the level of individual samples, allowing us to make more fine-grained assessments of where the model does and does not perform well.\n\n\nmetrics <- paths %>%\n  split(.$split) %>%\n  map(~ metrics_fun(.)) %>%\n  bind_rows(.id = \"split\")\nhead(metrics)\n\n\n# A tibble: 6 x 7\n  split threshold precision   recall class    ix path                 \n  <chr>     <dbl>     <dbl>    <dbl> <int> <int> <chr>                \n1 test      0.1     0.0189   3.62e-3     1     1 /Users/kris/data/pre…\n2 test      0.178   0.00941  8.91e-4     1     1 /Users/kris/data/pre…\n3 test      0.256   0.00825  5.57e-4     1     1 /Users/kris/data/pre…\n4 test      0.333   0.00641  3.34e-4     1     1 /Users/kris/data/pre…\n5 test      0.411   0.00404  1.67e-4     1     1 /Users/kris/data/pre…\n6 test      0.489   0.00174  5.57e-5     1     1 /Users/kris/data/pre…\n\nThe block below plots the metrics across samples. Each line corresponds to one training or test sample for a given class. The three classes are arranged in the separate columns – from left to right, they are clean-ice, debris-covered, and background class. No one threshold seems to stand out, with precision and recall curves varying smoothly across thresholds.\nPerformance on the debris-covered glaciers is noticeably worse than on the clean-ice ones. A few examples from each class are uniformly easier than others, but generally, the samples within a given class have comparable difficulty with one another.\nIt seems like we’ve overfit on the clean-ice glaciers, but underfit on the debris-covered ones. We can tell this because performance on training samples is noticeably better than for test samples for the clean-ice examples, but not for the debris-covered glaciers. In a future run, it might be worth rebalancing the data so that debris-covered glaciers are more common.\n\n\nmetrics_ <- metrics %>%\n  pivot_longer(precision:recall, names_to = \"metric\")\n\nggplot(metrics_, aes(x = threshold, y = value, col = split)) +\n  geom_line(aes(group = path), size = 0.5, alpha = 0.6) +\n  guides(col = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  facet_grid(metric ~ class) +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = element_rect(fill = \"NA\", size = .5)\n  )\n\n\n\n\nLet’s visualize a few samples that have especially good and bad predictions. To identify them, we compute the average precision and recall across all tested thresholds and then sort samples from best to worst.\n\n\nmetrics_ %>%\n  group_by(metric, path, class) %>%\n  summarise(mean = mean(value, na.rm = TRUE)) %>%\n  arrange(class, metric, desc(mean))\n\n\n# A tibble: 870 x 4\n# Groups:   metric, path [290]\n   metric    path                                          class  mean\n   <chr>     <chr>                                         <int> <dbl>\n 1 precision /Users/kris/data/predictions//train/y_hat-47…     1 0.808\n 2 precision /Users/kris/data/predictions//train/y_hat-67…     1 0.807\n 3 precision /Users/kris/data/predictions//test/y_hat-56.…     1 0.794\n 4 precision /Users/kris/data/predictions//train/y_hat-35…     1 0.772\n 5 precision /Users/kris/data/predictions//train/y_hat-39…     1 0.757\n 6 precision /Users/kris/data/predictions//train/y_hat-54…     1 0.754\n 7 precision /Users/kris/data/predictions//train/y_hat-38…     1 0.751\n 8 precision /Users/kris/data/predictions//train/y_hat-62…     1 0.714\n 9 precision /Users/kris/data/predictions//train/y_hat-31…     1 0.710\n10 precision /Users/kris/data/predictions//train/y_hat-26…     1 0.697\n# … with 860 more rows\n\nIt seems like sample 11 in the test split is one of the better ones. How does it look like? The block pulls the paths for x, y, and y_hat for this example and visualizes them. Try fiddling with the ix and split parameters to look at a few different examples.\n\n\nims <- paths %>%\n  filter(ix == 11, split == \"test\") %>% # what about 16 train? 4 test?\n  split(.$type) %>%\n  map(~ load_npy(.$path[1]))\n\np <- list(\n  plot_rgb(ims[[\"x\"]], c(5, 4, 2), r = 1, g = 2, b = 3),\n  plot_rgb(ims[[\"x\"]], c(13, 13, 13)),\n  plot_rgb(ims[[\"y_hat\"]], r = NULL),\n  plot_rgb(ims[[\"y\"]], r = NULL)\n)\ngrid.arrange(grobs = p, ncol = 2)\n\n\n\n\n\n\n\n",
      "last_modified": "2021-02-15T18:30:58-06:00"
    },
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-02-15T18:30:58-06:00"
    },
    {
      "path": "index.html",
      "title": "Visualizing Mapping Models",
      "description": "Data Science Bazaar Workshop <br/>\nFebruary 17, 2021\n",
      "author": [],
      "contents": "\n\nThis workshop explores the role of visualization in data science workflows that use satellite imagery. We’ll get hands-on experience with how visualization can support,\nData validation and cleaning\nModel inspection and error analysis.\nWe’ll walk through a toy version of a glacier mapping problem. Using freely available Landsat imagery, we’ll train a model to recognize different types of glaciers. You are encouraged to bring problems of your own for group discussion.\nPrerequisites\nBasic familiarity with R\nYou should be able to enter the binder notebooks linked here\nSchedule\nThis workshop will be held on February 17, 2021.\nTime\nFormat\nTopic\n2:45 - 3:00pm\nLecture\nVisualization and maps\n3:00 - 3:35pm\nInteractive Coding\nData exploration and preprocessing\n3:35 - 3:50pm\nBreakout Session\nDiscussion\n3:50 - 4:25\nInteractive Coding\nModel training and inspection\n4:25 - 4:45pm\nDebrief\nOpen Discussion\nResources\nIntroduction to Data Science: Chapter 7\nR for Data Science: Chapter 12\nGeographic Data in R: Chapter 2\nSpatial Data Science with R\nTowards better analysis of machine learning models: A visual analytics perspective\nContact\nDo you have any questions about the workshop? Don’t hesitate to reach out to ksankaran@wisc.edu.\n\n\n\n",
      "last_modified": "2021-02-15T18:30:59-06:00"
    },
    {
      "path": "materials.html",
      "title": "Materials",
      "description": "Data Science Bazaar Workshop <br/>\nFebruary 17, 2021\n",
      "author": [],
      "contents": "\nFollow Along\nRstudio\nJupyterlab\nGithub\nCompiled Notes\nData Preview\nPreprocess\nTrain\nSave\nEvaluate\nOther\nIntroductory slides\nDiscussion prompts\nWorkshop data, Full glacier data\n\n\n\n",
      "last_modified": "2021-02-15T18:30:59-06:00"
    }
  ],
  "collections": []
}
